version: '3.8'

services:
  # Database Layer
  postgres:
    image: pgvector/pgvector:pg15
    environment:
      POSTGRES_DB: cortexq
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d cortexq"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - rag-network

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - rag-network

  # MinIO Object Storage
  minio:
    image: minio/minio:latest
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin123
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - rag-network

  # Core API Service (All backend functionality)
  core-api:
    build: 
      context: ./core-api
      dockerfile: Dockerfile
    ports:
      - "8001:8000"
    environment:
      - DATABASE_URL=postgresql://admin:password@postgres:5432/cortexq
      - REDIS_URL=redis://redis:6379
      - JWT_SECRET=your-super-secret-jwt-key-change-in-production
      - DEBUG=true
      - OLLAMA_BASE_URL=http://ollama:11434
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin123
      - MINIO_SECURE=false
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio:
        condition: service_healthy
    volumes:
      # Map entire source directory for development
      - ./core-api/src:/app/src
      # Map scripts directory for development access
      - ./core-api/scripts:/app/scripts
      # Map alembic directory for migration development
      - ./core-api/alembic:/app/alembic
      - ./core-api/alembic.ini:/app/alembic.ini
      # Map requirements for dependency changes
      - ./core-api/requirements.txt:/app/requirements.txt
      # Map tests directory for test execution
      - ./tests:/app/tests
      # Persistent file storage
      - file_storage:/app/storage
    networks:
      - rag-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Ollama Model Initialization Service
  ollama-init:
    image: ollama/ollama:latest
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=ollama:11434
    networks:
      - rag-network
    command: >
      sh -c "
        echo 'Waiting for Ollama to be ready...' &&
        until curl -s http://ollama:11434/api/tags > /dev/null 2>&1; do
          echo 'Ollama not ready, waiting 5 seconds...'
          sleep 5
        done &&
        echo 'Ollama is ready! Checking for models...' &&
        if [ $$(curl -s http://ollama:11434/api/tags | grep -c '\"models\":\\[\\]') -eq 1 ]; then
          echo 'No models found, pulling llama3.2:1b...' &&
          ollama pull llama3.2:1b &&
          echo 'Model llama3.2:1b pulled successfully!'
        else
          echo 'Models already exist, skipping pull.'
        fi &&
        echo 'Ollama initialization complete!'
      "
    restart: "no"

  # Ollama for local LLM
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - rag-network
    environment:
      - OLLAMA_HOST=0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Frontend (Next.js) - Professional UI with Hot Reloading
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=development
    depends_on:
      - core-api
    networks:
      - rag-network
    volumes:
      # Map entire frontend directory for development
      - ./frontend:/app
      # Exclude node_modules to prevent conflicts
      - /app/node_modules
      # Exclude .next build directory
      - /app/.next

  # Bot Integration Service
  bot-service:
    build:
      context: ./services/ui/bot-service
      dockerfile: Dockerfile
    ports:
      - "8012:8012"
    environment:
      - RAG_SERVICE_URL=http://core-api:8000
      - CHAT_API_URL=http://core-api:8000
      - SLACK_BOT_TOKEN=${SLACK_BOT_TOKEN:-}
      - SLACK_SIGNING_SECRET=${SLACK_SIGNING_SECRET:-}
      - TEAMS_APP_ID=${TEAMS_APP_ID:-}
      - TEAMS_APP_PASSWORD=${TEAMS_APP_PASSWORD:-}
    depends_on:
      - core-api
    volumes:
      # Map bot service source for development
      - ./services/ui/bot-service/src:/app/src
      - ./services/ui/bot-service/package.json:/app/package.json
    networks:
      - rag-network

  # Nginx Reverse Proxy
  nginx:
    image: nginx:alpine
    ports:
      - "8080:80"
      - "8443:443"
    volumes:
      - ./config/nginx.conf:/etc/nginx/nginx.conf
      - ./config/ssl:/etc/nginx/ssl
    depends_on:
      - frontend
      - core-api
    networks:
      - rag-network

volumes:
  postgres_data:
  redis_data:
  file_storage:
  ollama_data:
  minio_data:

networks:
  rag-network:
    driver: bridge 