---
description: 
globs: 
alwaysApply: true
---
# Immediate Development Tasks

## Current Priority: Phase 1.1 - Project Foundation
Based on [TASK_LIST.md](mdc:docs/TASK_LIST.md), these are the immediate actionable tasks to start development.

## Today's Tasks (In Order)

### 1. Project Structure Setup âš¡ HIGH PRIORITY
```bash
# Create the complete directory structure
mkdir -p services/ingestion/{file-service,crawler-service,api-service}
mkdir -p services/search/{vector-service,schema-service,hybrid-service}
mkdir -p services/query/{classification-service,rag-service,agent-service}
mkdir -p services/ui/{chat-api,admin-service,bot-service}
mkdir -p services/infrastructure/{auth-service,audit-service,config-service}
mkdir -p config/{local,staging,production}
mkdir -p deployments/{docker,kubernetes}
mkdir -p scripts/{setup,deploy,backup}
mkdir -p docs/{api,architecture,deployment}
mkdir -p tests/{unit,integration,e2e}
```

### 2. Root Configuration Files âš¡ HIGH PRIORITY
Create these files in the project root:

#### `docker-compose.yml`
```yaml
version: '3.8'
services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: rag_searcher
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
  
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
  
  # Add services as they are developed

volumes:
  postgres_data:
```

#### `requirements.txt` (if using Python)
```txt
fastapi==0.104.1
uvicorn==0.24.0
sqlalchemy==2.0.23
psycopg2-binary==2.9.9
redis==5.0.1
pydantic==2.5.0
python-multipart==0.0.6
openai==1.3.7
```

#### `.env.example`
```env
# Database
DATABASE_URL=postgresql://admin:password@localhost:5432/rag_searcher
REDIS_URL=redis://localhost:6379

# LLM Configuration
OPENAI_API_KEY=your_openai_key_here
OLLAMA_BASE_URL=http://localhost:11434

# Security
JWT_SECRET_KEY=your_jwt_secret_here
ENCRYPTION_KEY=your_encryption_key_here

# Development
DEBUG=true
LOG_LEVEL=debug
```

#### `Makefile`
```makefile
.PHONY: setup install test dev clean

setup:
	@echo "Setting up development environment..."
	python -m venv venv
	./venv/bin/pip install -r requirements.txt
	cp .env.example .env

install:
	./venv/bin/pip install -r requirements.txt

dev:
	docker-compose up -d
	./venv/bin/uvicorn main:app --reload --host 0.0.0.0 --port 8000

test:
	./venv/bin/pytest tests/

clean:
	docker-compose down
	rm -rf venv/
```

### 3. Git Configuration ðŸ”§ MEDIUM PRIORITY
```bash
# Initialize git if not already done
git init

# Create .gitignore
echo "venv/
.env
__pycache__/
*.pyc
.pytest_cache/
.coverage
node_modules/
dist/
build/" > .gitignore

# Set up pre-commit hooks
pip install pre-commit
echo "repos:
- repo: https://github.com/psf/black
  rev: 23.11.0
  hooks:
  - id: black
- repo: https://github.com/pycqa/flake8
  rev: 6.1.0
  hooks:
  - id: flake8" > .pre-commit-config.yaml

pre-commit install
```

### 4. First Service: File Ingestion ðŸš€ START HERE
Begin with the most critical service:

#### Create `services/ingestion/file-service/src/main.py`
```python
from fastapi import FastAPI, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

app = FastAPI(title="File Ingestion Service", version="0.1.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "file-ingestion"}

@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    # Basic file upload endpoint
    return {
        "filename": file.filename,
        "content_type": file.content_type,
        "size": file.size,
        "status": "uploaded"
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001)
```

#### Create `services/ingestion/file-service/Dockerfile`
```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY src/ ./src/

EXPOSE 8001

CMD ["python", "src/main.py"]
```

### 5. Database Setup ðŸ’¾ MEDIUM PRIORITY
#### Create `scripts/setup/init_db.sql`
```sql
-- Create main database schema
CREATE EXTENSION IF NOT EXISTS vector;

-- Files table
CREATE TABLE files (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    filename VARCHAR(255) NOT NULL,
    content_type VARCHAR(100),
    size_bytes BIGINT,
    upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    processed BOOLEAN DEFAULT FALSE,
    metadata JSONB
);

-- Vector embeddings table
CREATE TABLE embeddings (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    source_id UUID REFERENCES files(id),
    embedding vector(1536),
    content_text TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Create indexes
CREATE INDEX idx_files_upload_date ON files(upload_date);
CREATE INDEX idx_embeddings_source_id ON embeddings(source_id);
```

## Next Week's Priorities

### Week 1: Core Foundation
- [ ] Complete file ingestion service with basic parsers
- [ ] Set up vector database with embedding generation
- [ ] Create basic chat API with WebSocket support
- [ ] Implement configuration service

### Week 2: Search and RAG
- [ ] Build vector search functionality
- [ ] Implement basic RAG pipeline
- [ ] Add hybrid search capabilities
- [ ] Create simple web UI for testing

### Week 3: Intelligence Layer
- [ ] Implement intent classification
- [ ] Build agent workflow system
- [ ] Add conversation context management
- [ ] Test end-to-end query processing

## Quick Commands to Start
```bash
# Run these commands to begin development
make setup
docker-compose up -d
cd services/ingestion/file-service && python src/main.py
```

## Success Criteria for Phase 1.1
- [ ] All directory structure created
- [ ] Docker environment running (PostgreSQL + Redis)
- [ ] File ingestion service accepting uploads
- [ ] Basic health checks working
- [ ] Git repository configured with hooks

**Focus**: Get the basic infrastructure running before adding complexity. Start simple, then iterate.
